{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T03:38:03.346410Z","iopub.execute_input":"2022-06-30T03:38:03.347481Z","iopub.status.idle":"2022-06-30T03:38:04.500589Z","shell.execute_reply.started":"2022-06-30T03:38:03.347378Z","shell.execute_reply":"2022-06-30T03:38:04.499348Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**SCENE SETTING**\n\nOne of the most well-known shipwrecks in history was the sinking of the RMS\nTitanic. RMS Titanic sunk on April 15, 1912, after striking an iceberg while on a\njourney. As a result of the lack of available lifeboats, a total of 1502 passengers\nand crew members perished.\n\n![titanic](https://robbreport.com/wp-content/uploads/2022/04/7.-titanic-sinking-Screen-Shot-2022-04-14-at-12.10.37-AM.jpg?w=1000)\n\nMore than 2200. Even if chance had a role, it appears that certain individuals\nhad a higher probability of surviving than others.\n\nSo in this project, there are two sets of Titanic passenger data: a training set and\na test set, both of which are.csv files. The \"Survived\" response variable, as well\nas 11 other 891-passenger informative factors, were included in the training\ndataset.\n\nThe goal of this project is to create a machine learning models in order to\nforecast which people survived the shipwreck. The response variable Survived\nwill be modeled in specific, given 10 different predictors. The rest of this paper\ngoes through the procedures that were used to create the predictive model.\nWe'll create models to forecast which individuals are more likely to survive. Also,\ncompare the models to see which is the most effective.\n","metadata":{}},{"cell_type":"markdown","source":"**Problem Analysis**\n\nAn examination of the Titanic's historical report provides useful insight into\nthe passenger data in terms of survival.\n\n1. Because of a \"women and children first\" protocol for filling lifeboats, a disproportionate number of men were left on board.\n\n2. Passengers in first and second class were the most likely to make it to the lifeboats. To get to the boat deck, third-class passengers had to navigate a tangle of passageways and staircases.\n\n3. Many lifeboats were barely partially loaded when they were deployed.\n\nOne of the most well-known shipwrecks in history was the sinking of the RMS Titanic. RMS Titanic sunk on April 15, 1912, after striking an iceberg while on a journey. As a result of the lack of available lifeboats, a total of 1502 passengers and crew members perished.\n\nMore than 2200. Even if chance had a role, it appears that certain individuals had a higher probability of surviving than others.\n\nSo in this project, there are two sets of Titanic passenger data: a training set and a test set, both of which are.csv files. The \"Survived\" response variable, as well as 11 other 891-passenger informative factors, were included in the training dataset.\n\nThe goal of this kaggle notebook is to create machine learning models in order to forecast which people survived the shipwreck. The response variable Survived will be modeled in specific, given 10 different predictors. The rest of this paper goes through the procedures that were used to create the predictive model.\n\nWe'll create models to forecast which individuals are more likely to survive. Also, compare the models to see which is the most effective.\n","metadata":{}},{"cell_type":"code","source":"#Performing EDA Analysis\n\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\n\nprint(\"Checking Test and Train Shapes :\")\nprint(\"Train DS Shape : \", train.shape)\nprint(\"Test DS Shape :  \", test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:04.502535Z","iopub.execute_input":"2022-06-30T03:38:04.502913Z","iopub.status.idle":"2022-06-30T03:38:04.532600Z","shell.execute_reply.started":"2022-06-30T03:38:04.502881Z","shell.execute_reply":"2022-06-30T03:38:04.531411Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"I performed a descriptive analysis of the dataset in order to gain maximum insight into the passengers aboard the Titanic. This analysis helped me identify things like gender ratio, people having different class tickets, age variance, etc.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:04.534353Z","iopub.execute_input":"2022-06-30T03:38:04.535174Z","iopub.status.idle":"2022-06-30T03:38:04.572490Z","shell.execute_reply.started":"2022-06-30T03:38:04.535129Z","shell.execute_reply":"2022-06-30T03:38:04.571400Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"But first, I identified what the given dataset contained. The values in the train dataset had the following structure and values","metadata":{}},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:04.575682Z","iopub.execute_input":"2022-06-30T03:38:04.576197Z","iopub.status.idle":"2022-06-30T03:38:04.587924Z","shell.execute_reply.started":"2022-06-30T03:38:04.576153Z","shell.execute_reply":"2022-06-30T03:38:04.586189Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:04.590270Z","iopub.execute_input":"2022-06-30T03:38:04.591345Z","iopub.status.idle":"2022-06-30T03:38:04.626195Z","shell.execute_reply.started":"2022-06-30T03:38:04.591269Z","shell.execute_reply":"2022-06-30T03:38:04.624680Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The dataset contained 5 categorical columns as shown below :","metadata":{}},{"cell_type":"code","source":"categorical_cols= train.select_dtypes(include=['object'])\nprint(f'The dataset contains {len(categorical_cols.columns.tolist())} categorical columns')\nfor cols in categorical_cols.columns:\n    print(cols,':', len(categorical_cols[cols].unique()),'labels')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:04.628341Z","iopub.execute_input":"2022-06-30T03:38:04.629192Z","iopub.status.idle":"2022-06-30T03:38:04.643822Z","shell.execute_reply.started":"2022-06-30T03:38:04.629143Z","shell.execute_reply":"2022-06-30T03:38:04.642675Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"But the main categorical data that can be identified are the fields of sex and embarked. Also, all of this categorical data needed to be converted into numerical data as many machine learning models are unable to work with categorical fields.","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:04.645493Z","iopub.execute_input":"2022-06-30T03:38:04.646154Z","iopub.status.idle":"2022-06-30T03:38:04.689762Z","shell.execute_reply.started":"2022-06-30T03:38:04.646118Z","shell.execute_reply":"2022-06-30T03:38:04.688601Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Also to be addressed were the missing values in the age and cabin fields as there are 891 rows in the training dataset. Using describe() we see the training dataset contained only 714 values for Age.\n\nI know from my historical research that women and children were given\npriority while loading lifeboats. As a result, I required a method of identifying\nchildren. Due to the enormous number of missing age data, this was a difficult\ntask.\nNext, I did a gender analysis to identify the male-female distribution on the ship.\nThe following results were observed:\n","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nbase_colors = ['#20618E',  '#6880AD',  '#57A7F3']\n\nlabels = [x for x in train.Sex.value_counts().index]\nvalues = train.Sex.value_counts()\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3,pull=[0.03, 0])])\n\nfig.update_layout(\n    title_text=\"Gender \")\nfig.update_traces(marker=dict(colors=base_colors))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:04.692030Z","iopub.execute_input":"2022-06-30T03:38:04.692505Z","iopub.status.idle":"2022-06-30T03:38:04.869208Z","shell.execute_reply.started":"2022-06-30T03:38:04.692460Z","shell.execute_reply":"2022-06-30T03:38:04.868121Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The total number of males was significantly higher. Keeping that in mind I also did\na gender vs survival rate analysis to confirm the assumption I gathered from\nhistorical report of the Titanic.","metadata":{}},{"cell_type":"code","source":"gender_analysis = sns.catplot(x=\"Sex\",y=\"Survived\",data=train, kind=\"bar\", height = 6, palette = base_colors)\ngender_analysis = gender_analysis.set_ylabels(\"Survival Rate\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:04.870699Z","iopub.execute_input":"2022-06-30T03:38:04.871047Z","iopub.status.idle":"2022-06-30T03:38:05.297380Z","shell.execute_reply.started":"2022-06-30T03:38:04.871017Z","shell.execute_reply":"2022-06-30T03:38:05.295908Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"From the above graph, it can be observed that even though the number of males was\nmuch higher than females, the survival rate of men was significantly lower. This also\nsupports my historical assumption about the Ladies and Children First protocol.","metadata":{}},{"cell_type":"markdown","source":"Next up is the analysis of the distribution of people in different passenger classes.\n\n\nC1 : First Class\n\nC2 : Second Class\n\nC3 : Third Class","metadata":{}},{"cell_type":"code","source":"labels = [x for x in train.Pclass.value_counts().index]\nvalues = train.Pclass.value_counts()\n\nfig = go.Figure(data=[go.Pie(labels=['C3','C1','C2'], values=values, hole=.3,pull=[0,0,0.04])])\n\nfig.update_layout(\n    title_text=\"Ticket class \")\nfig.update_traces(marker=dict(colors=base_colors))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:05.305077Z","iopub.execute_input":"2022-06-30T03:38:05.305975Z","iopub.status.idle":"2022-06-30T03:38:05.323224Z","shell.execute_reply.started":"2022-06-30T03:38:05.305922Z","shell.execute_reply":"2022-06-30T03:38:05.322067Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"According to the aforementioned analysis, the bulk of passengers are having a\nthird-class ticket. From the historical data, we know that third-class people had to\ntravel a lot in order to reach the safe area when compared to the people having\nfirst and second-class tickets. Keeping that in mind I also did a passenger class vs\nsurvival rate analysis on the basis of gender to confirm the assumption I gathered\nfrom the historical report of the Titanic","metadata":{}},{"cell_type":"code","source":"gender_pclass = sns.FacetGrid(train, height=4.5, aspect=1.6)\ngender_pclass.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', order=None, hue_order=None )\ngender_pclass.add_legend();","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:05.324656Z","iopub.execute_input":"2022-06-30T03:38:05.325673Z","iopub.status.idle":"2022-06-30T03:38:05.969123Z","shell.execute_reply.started":"2022-06-30T03:38:05.325582Z","shell.execute_reply":"2022-06-30T03:38:05.967756Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"According to the aforementioned analysis, females traveling in the third class had\na considerably lower survival rate. This backs up the historical assumption that\nfirst and second-class passengers were the most likely to make it to the lifeboats,\nwhich were hurriedly released partially loaded.\n\nNext up is the analysis of the distribution of people based on their port of\nembarkation.","metadata":{}},{"cell_type":"code","source":"labels = [x for x in train.Embarked.value_counts().index]\nvalues = train.Embarked.value_counts()\n\nfig=go.Figure(data=[go.Pie(labels=[\"Southampton\",\"Cherbourg\",\"Queenstown\"],values=values,hole=.3,pull=[0,0,0.06,0])])\n\nfig.update_layout(\n    title_text=\"Port of embarkation\")\nfig.update_traces(marker=dict(colors=base_colors))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:05.970783Z","iopub.execute_input":"2022-06-30T03:38:05.971094Z","iopub.status.idle":"2022-06-30T03:38:05.993950Z","shell.execute_reply.started":"2022-06-30T03:38:05.971066Z","shell.execute_reply":"2022-06-30T03:38:05.992643Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"According to the aforementioned analysis, it s observed that an overwhelming\nnumber of people embarked from Southampton also encoded as S.\n\nUpon further analysis, It was found out that apart from the missing values in the\nage variable, the age and fare values were positively skewed","metadata":{}},{"cell_type":"code","source":"import plotly.figure_factory as ff\nfrom plotly.offline import iplot\nage=train['Age'].dropna()\nfig = ff.create_distplot([age],['Age'],bin_size=1)\nfig.update_traces(marker=dict(color='#57A7F3'))\niplot(fig, filename='Basic Distplot')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:05.997398Z","iopub.execute_input":"2022-06-30T03:38:05.997920Z","iopub.status.idle":"2022-06-30T03:38:09.383101Z","shell.execute_reply.started":"2022-06-30T03:38:05.997869Z","shell.execute_reply":"2022-06-30T03:38:09.381730Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"fig = ff.create_distplot([train['Fare']],['Fare'],bin_size=10)\nfig.update_traces(marker=dict(color='#57A7F3'))\niplot(fig, filename='Basic Distplot')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.384912Z","iopub.execute_input":"2022-06-30T03:38:09.385391Z","iopub.status.idle":"2022-06-30T03:38:09.465598Z","shell.execute_reply.started":"2022-06-30T03:38:09.385341Z","shell.execute_reply":"2022-06-30T03:38:09.464397Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**FEATURE ENGINEERING**\n\nOne-Hot Encoding is a crucial step in preparing data for machine learning\nmodels. Many machine learning algorithms are unable to directly work on\ncategorical data such as gender (male/female). This implies that category\ndata must be transformed into numerical data. Feature Engineering is a\ncritical phase in designing any prediction system because the data may have\nmissing fields, incomplete fields, or fields containing secret information. Age,\nFare, and Embarked, for example, had missing values in the training and\ntesting data that needed to be filled up. I also used the passenger's surname\nto distinguish families on board the Titanic.\n\nFirst, I removed the name, passenger id, and ticket variables because they all\nhave unique values, and creating dummies for them would increase the\ndimensionality. The structure of train dataset after dropping values was:","metadata":{}},{"cell_type":"code","source":"train=train.drop(['PassengerId','Name','Ticket'],1)\ntest=test.drop(['PassengerId','Name','Ticket'],1)\n\ntrain['Survived']=train['Survived'].astype('int')\ntrain['Pclass']=train['Pclass'].astype('int')\ntrain['SibSp']=train['SibSp'].astype('int')\n\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.467397Z","iopub.execute_input":"2022-06-30T03:38:09.467750Z","iopub.status.idle":"2022-06-30T03:38:09.486248Z","shell.execute_reply.started":"2022-06-30T03:38:09.467718Z","shell.execute_reply":"2022-06-30T03:38:09.485289Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Now I addressed the missing values in the dataset. The following are the\npercentages of the missing values.","metadata":{}},{"cell_type":"code","source":"features=[features for features in train.columns if train[features].isnull().sum()>1]\nfor feature in features:\n    print(feature, np.round(train[feature].isnull().mean()*100, 2),  ' % missing values.\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.487184Z","iopub.execute_input":"2022-06-30T03:38:09.487502Z","iopub.status.idle":"2022-06-30T03:38:09.499993Z","shell.execute_reply.started":"2022-06-30T03:38:09.487472Z","shell.execute_reply":"2022-06-30T03:38:09.498944Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Because the cabin variable had over 77 percent missing values, I eliminated\nit. However, I can impute missing values as \"other\" by introducing another\ncategory. After doing so, only 2 categorical columns remained ie Sex and\nEmbarked .\n\nSince there were only two missing values in Embarked variable so I imputed\nthem with the mode of the rest of the values. Next to fill up the missing age\nvalues I used random values in between the 25th and 75th percentile. In the\ntest set there were also missing values for fare variable. Those were also\nimputed from the values range in the training set","metadata":{}},{"cell_type":"code","source":"train=train.drop(['Cabin'],1)\ntest=test.drop(['Cabin'],1)\n\ncategorical_cols_train= train.select_dtypes(include=['object'])\n\nprint(f'The dataset contains {len(categorical_cols_train.columns.tolist())} categorical columns')\n\ncategorical_cols_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.503399Z","iopub.execute_input":"2022-06-30T03:38:09.503814Z","iopub.status.idle":"2022-06-30T03:38:09.527642Z","shell.execute_reply.started":"2022-06-30T03:38:09.503768Z","shell.execute_reply":"2022-06-30T03:38:09.526772Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"categorical_cols_missing = categorical_cols_train.columns[categorical_cols_train.isnull().any()]\nfrom sklearn.impute import SimpleImputer\ncategoricalImputer = SimpleImputer(missing_values = np.NaN,strategy = 'most_frequent')\nfor feature in categorical_cols_missing:\n     categorical_cols_train[feature] = categoricalImputer.fit_transform(categorical_cols_train[feature].values.reshape(-1,1))\n     train[feature] = categoricalImputer.fit_transform(train[feature].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.529121Z","iopub.execute_input":"2022-06-30T03:38:09.529454Z","iopub.status.idle":"2022-06-30T03:38:09.798309Z","shell.execute_reply.started":"2022-06-30T03:38:09.529424Z","shell.execute_reply":"2022-06-30T03:38:09.797095Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train['Age'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.799819Z","iopub.execute_input":"2022-06-30T03:38:09.800300Z","iopub.status.idle":"2022-06-30T03:38:09.814948Z","shell.execute_reply.started":"2022-06-30T03:38:09.800251Z","shell.execute_reply":"2022-06-30T03:38:09.813587Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train['Fare'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.816472Z","iopub.execute_input":"2022-06-30T03:38:09.817620Z","iopub.status.idle":"2022-06-30T03:38:09.830524Z","shell.execute_reply.started":"2022-06-30T03:38:09.817531Z","shell.execute_reply":"2022-06-30T03:38:09.829325Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Now I had to convert the categorical data into numerical one. So the male\nand female of the sex field were encoded as 1 and 0 respectively.\nBecause the cabin variable had over 77 percent missing values, I eliminated\nit. However, I can impute missing values as \"other\" by introducing another\ncategory. After doing so, only 2 categorical columns remained ie Sex and\nEmbarked .\n\nSince there were only two missing values in Embarked variable so I imputed\nthem with the mode of the rest of the values. Next to fill up the missing age\nvalues I used random values in between the 25th and 75th percentile. In the\ntest set there were also missing values for fare variable. Those were also\nimputed from the values range in the training set.\n\nNext I had to create dummy variables for different Embarked classes like\nEmbarked_C, Embarked_Q, Embarked_S. Now these values will be binary\nencoded so that at a time only one of these values will be 1 signifying that\nthe given person embarked from that location. So for example a person\nembarked from a location C, then in the row, where that person's details\nare mentioned, the Embarked_C column will contain the value 1 while\nothers ie Embarked_Q and Embarked_S value will contain 0.\n","metadata":{}},{"cell_type":"code","source":"np.random.seed(1)\ntrain['Age'].fillna(np.random.randint(20,38), inplace = True)\ntest['Age'].fillna(np.random.randint(20,38), inplace = True)\ntest['Fare'].fillna(np.random.randint(0,31), inplace = True)\n\ncleanup_nums = {\"Sex\": {\"male\": 1, \"female\": 0}}\ntrain= train.replace(cleanup_nums)\ntest= test.replace(cleanup_nums)\n\ntrain=pd.get_dummies(train, columns=[\"Embarked\"])\ntest=pd.get_dummies(test, columns=[\"Embarked\"])\n\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.832146Z","iopub.execute_input":"2022-06-30T03:38:09.832863Z","iopub.status.idle":"2022-06-30T03:38:09.859327Z","shell.execute_reply.started":"2022-06-30T03:38:09.832823Z","shell.execute_reply":"2022-06-30T03:38:09.858098Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**CORRELATIONS IN THE DATASET**","metadata":{}},{"cell_type":"markdown","source":"Next, we move on to the correlations in the dataset.\n\nThrough the use of data correlations, we can determine how many variables and\nattributes are associated in your dataset. If one or more attributes are dependent\non another attribute, or if one or more attributes act as a catalyst for another\nattribute, we can infer this from correlation.\n\nIn this we analyze correlations between numerical and numerical variables,\nnumerical and categorical variables and categorical and categorical variables.\n","metadata":{}},{"cell_type":"code","source":"from scipy.stats import chi2_contingency\nimport numpy as np\n\ndef cramers_V(var1,var2):\n  crosstab =np.array(pd.crosstab(var1,var2, rownames=None, colnames=None)) # Cross table building\n  stat = chi2_contingency(crosstab)[0] # Keeping of the test statistic of the Chi2 test\n  obs = np.sum(crosstab) # Number of observations\n  mini = min(crosstab.shape)-1 # Take the minimum value between the columns and the rows of the cross table\n  return (stat/(obs*mini))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.860929Z","iopub.execute_input":"2022-06-30T03:38:09.861297Z","iopub.status.idle":"2022-06-30T03:38:09.868534Z","shell.execute_reply.started":"2022-06-30T03:38:09.861268Z","shell.execute_reply":"2022-06-30T03:38:09.867467Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"rows= []\n\nfor var1 in train:\n  col = []\n  for var2 in train:\n    cramers =cramers_V(train[var1], train[var2])\n    col.append(round(cramers,2))\n  rows.append(col)\n  \ncramers_results = np.array(rows)\ndf = pd.DataFrame(cramers_results, columns = train.columns, index =train.columns)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:38:09.869756Z","iopub.execute_input":"2022-06-30T03:38:09.870889Z","iopub.status.idle":"2022-06-30T03:38:11.457010Z","shell.execute_reply.started":"2022-06-30T03:38:09.870838Z","shell.execute_reply":"2022-06-30T03:38:11.455711Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt_data = train[:]\nsns.heatmap(df, vmin = -0.5,vmax = 1,annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:42:02.050215Z","iopub.execute_input":"2022-06-30T03:42:02.051435Z","iopub.status.idle":"2022-06-30T03:42:02.747733Z","shell.execute_reply.started":"2022-06-30T03:42:02.051383Z","shell.execute_reply":"2022-06-30T03:42:02.746592Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"To visualize the correlation matrix, I used a\nseaborn heatmap. The diagonal of the\ncorrelation map is all 1 which is because each\nvariable is correlated to itself.","metadata":{}},{"cell_type":"markdown","source":"**BUILDING PREDICTIVE MODELS**\n\n","metadata":{}},{"cell_type":"code","source":"model_results = []\n\nfrom sklearn.model_selection import GridSearchCV \ndef classification_eval (algorithm, grid_params, X_train, X_test, y_train, model_name) : \n    model = GridSearchCV(algorithm, grid_params, n_jobs = - 1, cv = 5, verbose = 1) \n    model. fit(X_train, y_train) \n    y_pred = model.predict (X_test) \n    print(\"Grid Search Best Score: \\t\", model.best_score_) \n    print(\"Grid Search Best Params: \\t\", model.best_params_)\n    model_results.append([model_name,model.best_score_])\n    return model, y_pred","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:44:20.533968Z","iopub.execute_input":"2022-06-30T03:44:20.534471Z","iopub.status.idle":"2022-06-30T03:44:20.543104Z","shell.execute_reply.started":"2022-06-30T03:44:20.534430Z","shell.execute_reply":"2022-06-30T03:44:20.541821Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"X_train = train.loc[:,['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked_C', 'Embarked_Q', 'Embarked_S']]\ny_train = train.loc[:,'Survived']\n\nX_test = test.loc[:,['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked_C', 'Embarked_Q', 'Embarked_S']]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:48:05.722209Z","iopub.execute_input":"2022-06-30T03:48:05.722672Z","iopub.status.idle":"2022-06-30T03:48:05.734143Z","shell.execute_reply.started":"2022-06-30T03:48:05.722637Z","shell.execute_reply":"2022-06-30T03:48:05.732994Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Balancing the dataset \nfrom imblearn.over_sampling import SMOTE \n\noversample = SMOTE()\nX_train, y_train = oversample.fit_resample(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:48:37.753972Z","iopub.execute_input":"2022-06-30T03:48:37.754387Z","iopub.status.idle":"2022-06-30T03:48:37.996909Z","shell.execute_reply.started":"2022-06-30T03:48:37.754355Z","shell.execute_reply":"2022-06-30T03:48:37.995598Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X_train,y_train,test_size=0.2,random_state=69)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:09:29.650101Z","iopub.execute_input":"2022-06-30T04:09:29.650638Z","iopub.status.idle":"2022-06-30T04:09:29.660649Z","shell.execute_reply.started":"2022-06-30T04:09:29.650598Z","shell.execute_reply":"2022-06-30T04:09:29.659577Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Classification using KNN\nfrom sklearn.neighbors import KNeighborsClassifier \ngrid_params = {\"weights\": ['uniform', 'distance'],\n                          'n_neighbors': range(2, 10, 2),\n                          \"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\n                          \"leaf_size\": range(15, 30, 5),\n                          \"p\" : range(1, 5),\n                          \"n_jobs\" : range(1,10, 2),\n                          }\nknn_model, y_pred_knn = classification_eval(KNeighborsClassifier(),grid_params,X_train, X_test, y_train,'KNN')\nknn_model","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:56:01.510100Z","iopub.execute_input":"2022-06-30T03:56:01.511003Z","iopub.status.idle":"2022-06-30T04:00:00.214688Z","shell.execute_reply.started":"2022-06-30T03:56:01.510950Z","shell.execute_reply":"2022-06-30T04:00:00.213368Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"knn_model.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:03:40.308988Z","iopub.execute_input":"2022-06-30T04:03:40.309416Z","iopub.status.idle":"2022-06-30T04:03:40.317874Z","shell.execute_reply.started":"2022-06-30T04:03:40.309384Z","shell.execute_reply":"2022-06-30T04:03:40.316714Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:10:59.189584Z","iopub.execute_input":"2022-06-30T04:10:59.189993Z","iopub.status.idle":"2022-06-30T04:10:59.195732Z","shell.execute_reply.started":"2022-06-30T04:10:59.189961Z","shell.execute_reply":"2022-06-30T04:10:59.194603Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model1 = KNeighborsClassifier(algorithm = 'ball_tree',\n leaf_size = 15,\n n_jobs = 1,\n n_neighbors = 6,\n p = 1,\n weights = 'distance')\nmodel1.fit(x_train, y_train)\n\nprint(\"train accuracy:\",model1.score(x_train, y_train),\"\\n\",\"test accuracy:\",model1.score(x_test,y_test))\n\nknnpred = model1.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(knnpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(model1, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:15:08.968525Z","iopub.execute_input":"2022-06-30T04:15:08.969962Z","iopub.status.idle":"2022-06-30T04:15:09.244018Z","shell.execute_reply.started":"2022-06-30T04:15:08.969907Z","shell.execute_reply":"2022-06-30T04:15:09.242819Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Classification using SVC\nfrom sklearn.svm import SVC \ngrid_params = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']}\nsvc_model, y_pred_svc = classification_eval(SVC(), grid_params, x_train, x_test, y_train,'SVC')\nsvc_model","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:17:23.271048Z","iopub.execute_input":"2022-06-30T04:17:23.271489Z","iopub.status.idle":"2022-06-30T04:17:28.905538Z","shell.execute_reply.started":"2022-06-30T04:17:23.271455Z","shell.execute_reply":"2022-06-30T04:17:28.904167Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"model1 = SVC(C = 1000, gamma = 0.001, kernel = 'rbf')\nmodel1.fit(x_train, y_train)\n\nprint(\"train accuracy:\",model1.score(x_train, y_train),\"\\n\",\"test accuracy:\",model1.score(x_test,y_test))\n\npred = model1.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(pred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(model1, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:19:21.256052Z","iopub.execute_input":"2022-06-30T04:19:21.256662Z","iopub.status.idle":"2022-06-30T04:19:21.888441Z","shell.execute_reply.started":"2022-06-30T04:19:21.256619Z","shell.execute_reply":"2022-06-30T04:19:21.886337Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier \ngrid_params = {'max_depth' : [1,5,10], 'min_samples_leaf' : [1,5,10], 'criterion':['gini','entropy'], } \n\ndt_model, y_pred_dt = classification_eval(DecisionTreeClassifier(), grid_params, x_train, x_test, y_train,'Decision Tree')\ndt_model","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:24:25.539473Z","iopub.execute_input":"2022-06-30T04:24:25.540013Z","iopub.status.idle":"2022-06-30T04:24:27.634809Z","shell.execute_reply.started":"2022-06-30T04:24:25.539972Z","shell.execute_reply":"2022-06-30T04:24:27.633647Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"model1 = DecisionTreeClassifier(criterion=\"gini\", max_depth=5, min_samples_leaf = 1)\nmodel1.fit(x_train, y_train)\n\nprint(\"train accuracy:\",model1.score(x_train, y_train),\"\\n\",\"test accuracy:\",model1.score(x_test,y_test))\n\npred = model1.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(pred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(model1, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:27:30.881302Z","iopub.execute_input":"2022-06-30T04:27:30.881795Z","iopub.status.idle":"2022-06-30T04:27:31.145988Z","shell.execute_reply.started":"2022-06-30T04:27:30.881756Z","shell.execute_reply":"2022-06-30T04:27:31.144455Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Classification using Random Forest\nfrom sklearn. ensemble import RandomForestClassifier \ngrid_params = {'criterion':['gini'],'n_estimators' : [100, 250, 500], 'max_depth' : [1,5,10,15,20], 'min_samples_leaf' : [1,2,5]} \n\nrf_model, y_pred_rf = classification_eval(RandomForestClassifier(), grid_params, x_train, x_test, y_train,'Random Forest')\nrf_model","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:29:00.247770Z","iopub.execute_input":"2022-06-30T04:29:00.248240Z","iopub.status.idle":"2022-06-30T04:29:56.987428Z","shell.execute_reply.started":"2022-06-30T04:29:00.248204Z","shell.execute_reply":"2022-06-30T04:29:56.986276Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"model1 = RandomForestClassifier(criterion=\"gini\", max_depth=20, min_samples_leaf = 2, n_estimators = 250)\nmodel1.fit(x_train, y_train)\n\nprint(\"train accuracy:\",model1.score(x_train, y_train),\"\\n\",\"test accuracy:\",model1.score(x_test,y_test))\n\npred = model1.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(pred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(model1, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:31:06.683238Z","iopub.execute_input":"2022-06-30T04:31:06.683950Z","iopub.status.idle":"2022-06-30T04:31:07.732983Z","shell.execute_reply.started":"2022-06-30T04:31:06.683905Z","shell.execute_reply":"2022-06-30T04:31:07.731780Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Classification using XGB Classifier\nfrom xgboost import XGBClassifier \ngrid_params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\nxg_model, y_pred_rf = classification_eval(XGBClassifier(use_label_encoder=False), grid_params, x_train, x_test, y_train,'XG Boost')\nxg_model","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:35:05.883776Z","iopub.execute_input":"2022-06-30T04:35:05.884389Z","iopub.status.idle":"2022-06-30T04:38:53.035959Z","shell.execute_reply.started":"2022-06-30T04:35:05.884336Z","shell.execute_reply":"2022-06-30T04:38:53.034495Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"xgb_model = XGBClassifier(colsample_bytree = 0.6, gamma = 0.5, max_depth = 5, min_child_weight =  5, subsample = 0.8, use_label_encoder=False)\nxgb_model.fit(x_train, y_train)\n\nprint(\"train accuracy:\",xgb_model.score(x_train, y_train),\"\\n\",\"test accuracy:\",xgb_model.score(x_test,y_test))\n\npred = xgb_model.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(pred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(model1, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:42:59.252513Z","iopub.execute_input":"2022-06-30T05:42:59.253049Z","iopub.status.idle":"2022-06-30T05:42:59.891915Z","shell.execute_reply.started":"2022-06-30T05:42:59.253012Z","shell.execute_reply":"2022-06-30T05:42:59.890022Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\ngrid_params = {\"penalty\":[\"l1\",\"l2\"], \"max_iter\" : [10, 100, 1000]}\nlr_model, y_pred_rf = classification_eval(LogisticRegression(), grid_params, x_train, x_test, y_train,'Logistic Regression')\nlr_model","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:19:22.308895Z","iopub.execute_input":"2022-06-30T05:19:22.309865Z","iopub.status.idle":"2022-06-30T05:19:22.759555Z","shell.execute_reply.started":"2022-06-30T05:19:22.309820Z","shell.execute_reply":"2022-06-30T05:19:22.758613Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(max_iter=2000,penalty='l2')\nmodel1=lr.fit(x_train, y_train)\nprint(\"train accuracy:\",model1.score(x_train, y_train),\"\\n\",\"test accuracy:\",model1.score(x_test,y_test))\nlrpred = lr.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for logistic regression\")\nprint(classification_report(lrpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for logistic regression\")\ndisplr = plot_confusion_matrix(lr, x_test, y_test,cmap=plt.cm.Blues , values_format='d')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:24:27.893205Z","iopub.execute_input":"2022-06-30T05:24:27.893754Z","iopub.status.idle":"2022-06-30T05:24:28.277272Z","shell.execute_reply.started":"2022-06-30T05:24:27.893715Z","shell.execute_reply":"2022-06-30T05:24:28.275938Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\ngrid_params = {\n    'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20],\n    'learning_rate': [(0.97 + x / 100) for x in range(0, 8)],\n    'algorithm': ['SAMME', 'SAMME.R']\n}\nada_model, y_pred_rf = classification_eval(AdaBoostClassifier(), grid_params, x_train, x_test, y_train,'Ada Boost')\nada_model","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:29:25.185899Z","iopub.execute_input":"2022-06-30T05:29:25.186638Z","iopub.status.idle":"2022-06-30T05:29:36.556894Z","shell.execute_reply.started":"2022-06-30T05:29:25.186581Z","shell.execute_reply":"2022-06-30T05:29:36.554776Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"ada=AdaBoostClassifier(algorithm = 'SAMME.R', learning_rate = 1.03, n_estimators = 12)\nmodel4=ada.fit(x_train, y_train)\nprint(\"train accuracy:\",model4.score(x_train, y_train),\"\\n\",\"test accuracy:\",model4.score(x_test,y_test))\nadapred = ada.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(adapred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(ada, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:30:56.496220Z","iopub.execute_input":"2022-06-30T05:30:56.496786Z","iopub.status.idle":"2022-06-30T05:30:56.787740Z","shell.execute_reply.started":"2022-06-30T05:30:56.496742Z","shell.execute_reply":"2022-06-30T05:30:56.786380Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import RidgeClassifier\ngrid_params = {'alpha':[1, 10]}\nrid_model, y_pred_rf = classification_eval(RidgeClassifier(), grid_params, x_train, x_test, y_train,'Ridge Boost')\nrid_model","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:34:09.236275Z","iopub.execute_input":"2022-06-30T05:34:09.236860Z","iopub.status.idle":"2022-06-30T05:34:09.346344Z","shell.execute_reply.started":"2022-06-30T05:34:09.236813Z","shell.execute_reply":"2022-06-30T05:34:09.344756Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"rc =RidgeClassifier()\nmodel5=rc.fit(x_train, y_train)\nprint(\"train accuracy:\",model5.score(x_train, y_train),\"\\n\",\"test accuracy:\",model5.score(x_test,y_test))\nrcpred = rc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for Ridge Classification\")\nprint(classification_report(rcpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for Ridge Regression\")\ndisplr = plot_confusion_matrix(rc, x_test, y_test,cmap=plt.cm.Blues , values_format='d')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:34:27.797071Z","iopub.execute_input":"2022-06-30T05:34:27.797528Z","iopub.status.idle":"2022-06-30T05:34:28.050677Z","shell.execute_reply.started":"2022-06-30T05:34:27.797492Z","shell.execute_reply":"2022-06-30T05:34:28.049286Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"results = pd.DataFrame(model_results,columns = ['Model','Best Score'])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:36:14.424730Z","iopub.execute_input":"2022-06-30T05:36:14.425296Z","iopub.status.idle":"2022-06-30T05:36:14.434396Z","shell.execute_reply.started":"2022-06-30T05:36:14.425257Z","shell.execute_reply":"2022-06-30T05:36:14.432802Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"# results = results.drop([0, 6, 7, 8, 9, 10, 11, 13, 15])\nresults","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:36:37.511767Z","iopub.execute_input":"2022-06-30T05:36:37.512337Z","iopub.status.idle":"2022-06-30T05:36:37.527643Z","shell.execute_reply.started":"2022-06-30T05:36:37.512296Z","shell.execute_reply":"2022-06-30T05:36:37.526643Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"**GENERATING SUBMISSION VIA XGBOOST**","metadata":{}},{"cell_type":"code","source":"test2 = pd.read_csv('/kaggle/input/titanic/test.csv')\nx = test2[\"PassengerId\"]\npred = xgb_model.predict(test)\nsubmission = pd.DataFrame({\"PassengerId\" : x, \"Survived\" : pred})\nsubmission.to_csv('submission.csv', index = False, encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T05:53:24.641821Z","iopub.execute_input":"2022-06-30T05:53:24.642478Z","iopub.status.idle":"2022-06-30T05:53:24.703673Z","shell.execute_reply.started":"2022-06-30T05:53:24.642435Z","shell.execute_reply":"2022-06-30T05:53:24.702336Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"**CONCLUSION**\n\nThe role of machine learning applications in disaster management and\npredictions has been increasing rapidly over the past years. Following this\ntrend, I was given an assignment to predict the survival of passengers aboard\nthe infamous Titanic Ship sailing across the North Pacific Region.\nAs a consequence of my efforts, I obtained important knowledge in the\ndevelopment of prediction algorithms and set a high of 84 percent accuracy\nin the \"Titanic - Machine Learning from Disaster\" competition organized by\nKaggle.\n\nFrom my work in building machine learning models to predict the survival of\npassengers aboard the titanic, I'd rather be a young female with a first class\nticket.\n","metadata":{}},{"cell_type":"markdown","source":"**If you liked my work, or want to give me any suggesstions on how I can improve upon it, do contact me via the follwoing links **\n\n**Github: https://github.com/AkshatRastogi-1nC0re **\n** LinkedIn: https://www.linkedin.com/in/akshat-rastogi-3425aa1b8/ **\n\n**SIGNING OUT **\n** AKSHAT RASTOGI**","metadata":{}}]}